{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI820A_Bp_NH"
      },
      "source": [
        "# Assignment 3\n",
        "\n",
        "\n",
        "\n",
        "# Write your name and simon email\n",
        "\n",
        "Please write names below\n",
        "\n",
        "1. [Name, student ID, email]:\n",
        "2. [Name, student ID, email]:\n",
        "2. [Name, student ID, email]:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekyShyt91xTn"
      },
      "source": [
        "# Exercises \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWZRPwCw1xTo"
      },
      "source": [
        "**1. Data Cleaning**\n",
        "\n",
        "You will work with the same dataset as in Assigment 1.\n",
        "The dataset has address \n",
        "\n",
        "`url='https://github.com/amoreira2/Fin418/blob/main/assets/data/Assignment1.xlsx?raw=true'`\n",
        "\n",
        "Do the followings:\n",
        "\n",
        "- Import pandas, numpy, matplotlib, and load the data set.\n",
        "- Import the datasets of industry returns and risk free rate.\n",
        "- Parse the date.\n",
        "- Set the index.\n",
        "- Drop missing observations.\n",
        "- Construct a dataframe with only excess returns.\n",
        "- Call this dataframe with the 49 excess returns time series `df`.\n",
        "- Call `df.head()` to check if everything works\n",
        "\n",
        "**_Hint:_**\n",
        "You did it in assignment 1 (the dataframe name was different), simply copy and paste your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "from pandas.tseries.offsets import MonthEnd\n",
        "\n",
        "# your code below\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ke8n1Nw1xTq"
      },
      "source": [
        "**2. Expected excess return estimation**\n",
        "\n",
        "Compute the sample mean as the estimators for the expected excess returns of the 49 assets. \n",
        "\n",
        "Call this `ERe`.\n",
        "\n",
        "**_Hint:_**\n",
        "Use risk-free rates from the \"Market_proxy\" sheet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code below\n",
        "\n",
        "ERe.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7wvamWs1xTr"
      },
      "source": [
        "**3. Expected excess return uncertainty**\n",
        "\n",
        "We will now construct an estimator for the amount of uncertainty in our sample mean estimator. If we assume that each individual asset is uncorrelated over time (not terrible assumption), then the variance of the mean is\n",
        "\n",
        "$$var(\\bar{r_i}) = var(\\frac{\\sum_t^T r_{i,t}}{T})=\\frac{\\sum_t^T var( r_{i,t})}{T^2} = \\frac{var(r_{i,t})}{T}$$\n",
        "\n",
        "So all you need is the sample size (T) and the variance of each asset to obtain the varaince of our estimator.\n",
        "\n",
        "Please use this formula to compute the **STANDARD DEVIATION** of sample average estimator of each 49 asset. Call this `ERe_se`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code below\n",
        "\n",
        "ERe_se.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJi0bAYO1xTs"
      },
      "source": [
        "**4. Constructing a confidence interval for the expected excess return, part 1**\n",
        "\n",
        "We will now want to construct the 95% confidence interval for our estimator. The interval is such that it contains the true mean 95% of the time.\n",
        "\n",
        "The way to do this is to use the normal distribution CDF to figure out the threshold that leaves only 2.5% probability at the each side of the tails.\n",
        "\n",
        "Why 2.5% and not 5%? Because it is symmetric, so there is 2.5% probability in the left tail and 2.5 % in the right tail so overall there is only 5% probability that the expected return is out of the interval. Thus there is 95% probability that it is in the interval.\n",
        "\n",
        "In this exercise, you will find the threshold by doing the followings:\n",
        "\n",
        "1. import the stats library from the scipy package with `from scipy import stats`\n",
        "\n",
        "2. get the standard normal distribution with `sn=stats.norm(0,1)`, where 0 is the mean and 1 is the standard deviation\n",
        "\n",
        "3. get the threshold by using inverse cumulative distribution function for the appropriate `prob_value` to create a 95% CI (see discussion above). `threshold=sn.isf(prob_value)`\n",
        "\n",
        "4. make sure that this threshold is positive (if you got from the left tail, you will have to take the absolute value or just get from the right tail).\n",
        "\n",
        "5. make sure you did things correctly by calling `print(threshold)`.\n",
        "\n",
        "**_Hint:_**\n",
        "\n",
        "You can always check in the normal table that you did things correctly https://en.wikipedia.org/wiki/Normal_distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code below\n",
        "\n",
        "print(threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q0HgrRl1xTu"
      },
      "source": [
        "**5. Constructing a confidence interval for the expected excess return, part 2**\n",
        "\n",
        "Armed with these threshold you can construct the interval as follows\n",
        "\n",
        "$$[\\bar{r}-threshold\\times\\sigma(\\bar{r}),\\bar{r}+threshold\\times\\sigma(\\bar{r})]$$\n",
        "\n",
        "Do the followings:\n",
        "\n",
        "1. create an empty dataframe which has the names of industries as index and 'lower' and 'upper' as column names. Name it `ERe_ci`.\n",
        "\n",
        "2. construct the lower bound of the interval, $\\bar{r}-threshold\\times\\sigma(\\bar{r})$, and store it in the column of 'lower'\n",
        "\n",
        "3. compute the upper bound symmetrically, and store it in the column of 'upper'\n",
        "\n",
        "4. call `ERe_ci.head()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code below\n",
        "\n",
        "ERe_ci.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIWJeB2S1xTv"
      },
      "source": [
        "**6. Compute the tangency portfolio weights for a portfolio with annualized volatility of 10%**\n",
        "\n",
        "A portfolio that is very important is the Mean-Variance-Efficient portfolio--sometimes also called the Tangency portfolio. This portfolio plays an outsized role in finance theory--for example any model of asset prices can be characterized as a prediction for which portfolio is MVE--for example in the case of the CAPM it predicts the market portfolio to be MVE.\n",
        "\n",
        "For us we will simply discuss it's empirical implementation. It turns out that if you know the true moments of the excess returns distributions of the assets, $E[R^e]$  and $Var(R^e)$, then the portfolio that maximizes the SR is any portfolio weight proportional to \n",
        "\n",
        "$$Var(R^e)^{-1}E[R^e]$$\n",
        "\n",
        "So for any number x, the portfolio $W=xVar(R^e)^{-1}E[R^e]$ has the highest feasible sharpe ratio.\n",
        "\n",
        "Let's construct this portfolio using the sample moments, so it will return the static portfolio that had the highest sharpe ratio in the sample.\n",
        "\n",
        "1. you already estimated E[Re]\n",
        "2. Now use .cov() to estimate the variance-covariance matrix and save it as `CovRe`\n",
        "3. Invert the variance covariance matrix usign np.linalg.inv()\n",
        "4. Construct the weights\n",
        "5. Choose x so that the weights add up to 1\n",
        "\n",
        "Store these in a dataframe whose rows are the names of the assets and the first column has the label 'mve_data'.\n",
        "\n",
        "Name this data frame `Weights`.\n",
        "\n",
        "`print(Weights)`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code below\n",
        "\n",
        "# Your CovRe\n",
        "CovRe = \n",
        "\n",
        "# Plug in appropriate variables in `invCovRe` and 'ERe'\n",
        "raw_weights = 'invCovRe' @ 'ERe'\n",
        "\n",
        "# Normalize weights to sum to 1\n",
        "x = \n",
        "norm_weights = x * raw_weights\n",
        "\n",
        "# Store in DataFrame\n",
        "Weights = \n",
        "\n",
        "print(Weights.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOSnMt-p1xTv"
      },
      "source": [
        "**7. Sensitivity to uncertainty of the Tangent portfolio calculation**\n",
        "\n",
        "Now we will compute the tangency portfolio but using a slightly different estimate for the mean. \n",
        "\n",
        "Do the followings.\n",
        "\n",
        "1. instead of using the sample mean for each asset, first we will pick one asset, `Hshld`. \n",
        "\n",
        "2. change its mean to be its lower bound of CI (confidence interval) in Exercise 5 and then recalculate the tangency portfolio weights. (That is, change the mean of `Hshld` ONLY and go through the same process as exercise 6 to recalculate the portfolio weights for ALL assets.)\n",
        "\n",
        "3. store this in dataframe `Weights` with the column name `mve_Hlth-1.95`.\n",
        "\n",
        "4. create another column with weights computed from the perturbation in which the mean of `Hshld` is changed to be the upper bound of CI, label this column `mve_Hshld+1.95 `. \n",
        "\n",
        "5. do a bar plot of these three sets of weights using Weights.plot.bar().\n",
        "\n",
        "Discuss what you notice in the bar plot:\n",
        "\n",
        "1. How much do the weights change?\n",
        "\n",
        "2. Which assets are impacted? Why?\n",
        "\n",
        "**_Hint:_** \n",
        "\n",
        "you might want to create a copy of your ERe estimator before you do the perturbation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code below\n",
        "\n",
        "# Make copies of `ERe` dataframe and name them `ERe_lower` and 'ERe_upper'\n",
        "ERe_lower = ERe.copy()\n",
        "ERe_upper = ERe.copy()\n",
        "\n",
        "# Replace Hshld's mean with lower and upper CI bounds\n",
        "ERe_lower['Hshld'] = \n",
        "ERe_upper['Hshld'] = \n",
        "\n",
        "# Compute weights as in the above exercise and save the values of each in `Weights` dataframe\n",
        "\n",
        "# Bar plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5XhSWJl1xTw"
      },
      "source": [
        "your discussion below\n",
        "\n",
        "1. How much do the weights change?\n",
        "\n",
        "2. Which assets are impacted? Why?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0HUt9Od1xTw"
      },
      "source": [
        "**8. Performance impact of estimation uncertainty**\n",
        "\n",
        "\n",
        "Your `Weight` dataframe has 3 different weight schemes (three columns).\n",
        "\n",
        "Do the followings:\n",
        "\n",
        "1. compute the in-sample Sharpe Ratio for these 3 different weight schemes.\n",
        "\n",
        "2. discuss the results that you obtained\n",
        "\n",
        "**_Hint:_**\n",
        "You should use the **real** data (e.g. `df`,`ERe`,`CovRe` ) to compute the Sharpe ratio (not the perturbed means in exercise 7)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFhbKApV1xTx"
      },
      "source": [
        "your discussion below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GksOAMW1xTx"
      },
      "source": [
        "**9. Reproduce analysis of Exercises 7-8 for all assets**\n",
        "\n",
        "Do the followings:\n",
        "\n",
        "1. use a for loop to loop through the 49 portfolios and create the \"perturbed\" weights and the Sharpe Ratio of the perturbed weights by performing the same exercises as in exercise 7 and 8. \n",
        "\n",
        "2. record for each asset the average drop in the Sharpe Ratio associated with the perturbation in the tangency portfolio weights. \n",
        "\n",
        "3. store the results in a dataframe named `dSR` (difference in SR):\n",
        "$$dSR[asset]=\\frac{1}{2}\\frac{SR(asset+1.95)-SR+SR(asset-1.95)-SR}{SR(data)}$$\n",
        "where SR(asset+1.95) and SR(asset-1.95) were the Sharpe ratios obtained when you perturb the expected excess return of that asset to the upper and lower bound of the CI.\n",
        "`dSR` should be a dataframe with industry names as index and a column, called `SR_change`, containing the results of the calculation from the expression above.\n",
        "\n",
        "4. do a bar plot of this Sharpe ratio change.\n",
        "\n",
        "Discuss the bar plot:\n",
        "\n",
        "1. What do you think is the key takeaway from the analysis above\n",
        "\n",
        "***Hint:***\n",
        "Note that all you need to do here is to get the code you developed above and adapt it to work with a for loop.\n",
        "\n",
        "***Hint:***\n",
        "When saving into `dSR`, You can use .from_dict method of pd.DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your code below\n",
        "    \n",
        "# It's useful to use dictionary for this exercise\n",
        "sr_changes = {}\n",
        "\n",
        "# For each asset, do the same exercise\n",
        "for asset in ERe.index:\n",
        "    # Write a code calculating Sharpe ratios using above exercise\n",
        "\n",
        "    # Compute average SR drop. Hint: Use the formula in the problem.\n",
        "    sr_diff = \n",
        "    sr_changes[asset] = sr_diff\n",
        "\n",
        "# Save it into DataFrame\n",
        "dSR = \n",
        "\n",
        "# Bar plot\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY2TuV-x1xTy"
      },
      "source": [
        "your discussion below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp9iH3Nm1xTy"
      },
      "source": [
        "**10. Monte Carlo, part 1**\n",
        "\n",
        "So far our focus is on the estimation uncertainty of risk premiums. Covariance matrices also need to be estimated.\n",
        "\n",
        "You will now implement a Monte-Carlo method to evaluate the overall uncertainty in the construction of the tangency portfolio.\n",
        "\n",
        "You already have the sample estimates from the vector of expected excess returns `ERe` and the variance-covariance matrix `CovRe`  (you used those in Exercise 6).\n",
        "\n",
        "Now you use the function `np.random.multivariate_normal` to simulate draws from a multivariate normal distribution with vector of mean equal to `ERe` and the covariance matrix equal to `CovRe`.\n",
        "\n",
        "Do the following:\n",
        "\n",
        "1. write the code that draws ONE realization of returns for this set of 49 assets. \n",
        "\n",
        "***Hint:***\n",
        "\n",
        "you should get a vector 49 by 1 that changes every time you run the cell.\n",
        "\n",
        "type `np.random.multivariate_normal?` to see how this function works\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku2xyycV1xTy"
      },
      "outputs": [],
      "source": [
        "# your code below\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD33YJYC1xTy"
      },
      "source": [
        "**11. Monte Carlo, part 2**\n",
        "\n",
        "Do the followings:\n",
        "\n",
        "1. now set the parameter `size` in the `multivaraite_normal` function to draw T realizations of the 49 assets, where you set T to the number of months you have in the data set.\n",
        "\n",
        "2. print the shape of your draw. This should return you a T by N matrix of returns, something with exactly the same shape as our data set. \n",
        "\n",
        "***Hint:***\n",
        "every time you run the cell again you get a different realization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BIZ7lNp1xTy"
      },
      "outputs": [],
      "source": [
        "# your code below\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8E6w9nL1xTz"
      },
      "source": [
        "**12. Monte Carlo, part 3**\n",
        "\n",
        "Do the followings:\n",
        "\n",
        "1. copy the code above, so you have a simulated sample of monthly industry returns.\n",
        "\n",
        "2. use the simulated return data and the weights in `mve_data` column of the dataframe `Weights` to construct a time-series of portfolio excess return. That is use the weights you constructed using the sample moments of the original sample.\n",
        "\n",
        "3. compute and print its Sharpe Ratio. \n",
        "\n",
        "***Hint:***\n",
        "\n",
        "Every time you run this cell you should get a different Sharpe Ratio. This variation reflects the amount of overall uncertainty built in our investment strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8Z3CiIX1xTz"
      },
      "outputs": [],
      "source": [
        "# your code below\n",
        "simulated_returns = \n",
        "w_mve = Weights['mve_data'].values\n",
        "\n",
        "# Compute time-series of simulated portfolio returns\n",
        "simulated_port_returns = \n",
        "\n",
        "# Compute Sharpe Ratio of the simulated portfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgVHy9TC1xTz"
      },
      "source": [
        "**13. Monte Carlo, part 4**\n",
        "\n",
        "Now copy the code of the question above and write a for loop around it. \n",
        "\n",
        "Do the followings:\n",
        "\n",
        "1. loop throught this code 1000 times and each time record the resulting Sharpe Ratio.\n",
        "\n",
        "2. save this in a dataframe called `MC`.\n",
        "\n",
        "3. create a histogram of these Sharpe Ratios with 50 bins using the method `.hist`\n",
        "\n",
        "Discuss the plot:\n",
        "\n",
        "1. what do you conclude?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ey7Znsls1xTz"
      },
      "outputs": [],
      "source": [
        "# your code below\n",
        "\n",
        "# Prepare saving space\n",
        "mc_sharpes = []\n",
        "\n",
        "# Run simulation 1000 times\n",
        "for iter in range(1000):\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vMNnh711xTz"
      },
      "source": [
        "your discussion below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExscHOuF1xT0"
      },
      "source": [
        "**14. Please explain why an investor should care about these results.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvL1hqr91xT1"
      },
      "source": [
        "your answer below\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "toc-autonumbering": true
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
